<?xml version='1.0' encoding='utf-8'?>
<scheme description="" title="" version="2.0">
	<nodes>
		<node id="0" name="Import Images" position="(189.0, 213.0)" project_name="Orange3-ImageAnalytics" qualified_name="orangecontrib.imageanalytics.widgets.owimageimport.OWImportImages" title="Import Images" version="" />
		<node id="1" name="Data Table" position="(307.0, 97.0)" project_name="Orange3" qualified_name="Orange.widgets.data.owtable.OWDataTable" title="Data Table" version="" />
		<node id="2" name="Python Script" position="(485.0, 523.0)" project_name="Orange3" qualified_name="Orange.widgets.data.owpythonscript.OWPythonScript" title="Python Script" version="" />
		<node id="3" name="Image Embedding" position="(286.0, 307.0)" project_name="Orange3-ImageAnalytics" qualified_name="orangecontrib.imageanalytics.widgets.owimageembedding.OWImageEmbedding" title="Image Embedding" version="" />
		<node id="4" name="Logistic Regression" position="(309.0, 530.0)" project_name="Orange3" qualified_name="Orange.widgets.model.owlogisticregression.OWLogisticRegression" title="Logistic Regression" version="" />
		<node id="5" name="Confusion Matrix" position="(530.0, 370.0)" project_name="Orange3" qualified_name="Orange.widgets.evaluate.owconfusionmatrix.OWConfusionMatrix" title="Confusion Matrix" version="" />
		<node id="6" name="Predictions" position="(487.0, 648.0)" project_name="Orange3" qualified_name="Orange.widgets.evaluate.owpredictions.OWPredictions" title="Predictions" version="" />
	</nodes>
	<links>
		<link enabled="true" id="0" sink_channel="Data" sink_node_id="1" source_channel="Data" source_node_id="0" />
		<link enabled="true" id="1" sink_channel="Images" sink_node_id="3" source_channel="Data" source_node_id="0" />
		<link enabled="true" id="2" sink_channel="Learner" sink_node_id="2" source_channel="Learner" source_node_id="4" />
		<link enabled="true" id="3" sink_channel="Data" sink_node_id="4" source_channel="Embeddings" source_node_id="3" />
	</links>
	<annotations />
	<thumbnail />
	<node_properties>
		<properties format="pickle" node_id="0">gASVkAEAAAAAAAB9lCiMEmNvbnRyb2xBcmVhVmlzaWJsZZSIjAxyZWNlbnRfcGF0aHOUXZSMHm9y
YW5nZXdpZGdldC51dGlscy5maWxlZGlhbG9nc5SMClJlY2VudFBhdGiUk5QpgZR9lCiMB2Fic3Bh
dGiUjHpDOi9Vc2Vycy9NRS9PbmVEcml2ZS9Eb2N1bWVudHMvU2Nob29sL1BTVS9GYWxsMjAyMC9F
Q0U1NzgvQXNzaWdubWVudHMvQXNzaWdubWVudDMvZ2VzdHVyZS1yZWNvZ25pdGlvbi1tYXN0ZXIv
VHJhaW5HZXN0dXJlc5SMBnByZWZpeJROjAdyZWxwYXRolE6MBXRpdGxllIwAlIwFc2hlZXSUaA6M
C2ZpbGVfZm9ybWF0lE51YmGME3NhdmVkV2lkZ2V0R2VvbWV0cnmUQ0IB2dDLAAMAAAAAAbIAAAE0
AAAC7AAAAdUAAAGzAAABUwAAAusAAAHUAAAAAAAAAAAHgAAAAbMAAAFTAAAC6wAAAdSUjAtfX3Zl
cnNpb25fX5RLAXUu
</properties>
		<properties format="literal" node_id="1">{'auto_commit': True, 'color_by_class': True, 'controlAreaVisible': True, 'dist_color_RGB': (220, 220, 220, 255), 'savedWidgetGeometry': b'\x01\xd9\xd0\xcb\x00\x03\x00\x00\x00\x00\x02/\x00\x00\x00\xf4\x00\x00\x06\xa1\x00\x00\x03\x07\x00\x00\x020\x00\x00\x01\x13\x00\x00\x06\xa0\x00\x00\x03\x06\x00\x00\x00\x00\x00\x00\x00\x00\x07\x80\x00\x00\x020\x00\x00\x01\x13\x00\x00\x06\xa0\x00\x00\x03\x06', 'select_rows': True, 'selected_cols': [0, 1, 2, 3, 4, 5], 'selected_rows': [2], 'show_attribute_labels': True, 'show_distributions': False, '__version__': 1}</properties>
		<properties format="literal" node_id="2">{'controlAreaVisible': True, 'currentScriptIndex': 1, 'savedWidgetGeometry': b'\x01\xd9\xd0\xcb\x00\x03\x00\x00\x00\x00\x03\xc0\x00\x00\x00\x00\x00\x00\x07\x7f\x00\x00\x04\x19\x00\x00\x03\xc1\x00\x00\x00\x1f\x00\x00\x07~\x00\x00\x04\x18\x00\x00\x00\x00\x00\x00\x00\x00\x07\x80\x00\x00\x03\xc1\x00\x00\x00\x1f\x00\x00\x07~\x00\x00\x04\x18', 'scriptLibrary': [{'name': 'Hello world', 'script': '#------------------------------------------------------------\n# SEGMENT, RECOGNIZE and COUNT fingers from a video sequence\n#------------------------------------------------------------\n\n# organize imports\nimport cv2\nimport imutils\nimport numpy as np\nfrom sklearn.metrics import pairwise\n\n\n# global variables\nbg = None\n\n#--------------------------------------------------\n# To find the running average over the background\n#--------------------------------------------------\ndef run_avg(image, accumWeight):\n    global bg\n    # initialize the background\n    if bg is None:\n        bg = image.copy().astype("float")\n        return\n\n    # compute weighted average, accumulate it and update the background\n    cv2.accumulateWeighted(image, bg, accumWeight)\n\n#---------------------------------------------\n# To segment the region of hand in the image\n#---------------------------------------------\ndef segment(image, threshold=24):\n    global bg\n    # find the absolute difference between background and current frame\n    diff = cv2.absdiff(bg.astype("uint8"), image)\n\n    # threshold the diff image so that we get the foreground\n    thresholded = cv2.threshold(diff, threshold, 128, cv2.THRESH_BINARY)[1]\n\n    # get the contours in the thresholded image\n    (cnts, _) = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    # return None, if no contours detected\n    if len(cnts) == 0:\n        return\n    else:\n        # based on contour area, get the maximum contour which is the hand\n        segmented = max(cnts, key=cv2.contourArea)\n        return (thresholded, segmented)\n\n#--------------------------------------------------------------\n# To count the number of fingers in the segmented hand region\n#--------------------------------------------------------------\ndef count(thresholded, segmented):\n    # find the convex hull of the segmented hand region\n    chull = cv2.convexHull(segmented)\n\n    # find the most extreme points in the convex hull\n    extreme_top    = tuple(chull[chull[:, :, 1].argmin()][0])\n    extreme_bottom = tuple(chull[chull[:, :, 1].argmax()][0])\n    extreme_left   = tuple(chull[chull[:, :, 0].argmin()][0])\n    extreme_right  = tuple(chull[chull[:, :, 0].argmax()][0])\n\n    # find the center of the palm\n    cX = int((extreme_left[0] + extreme_right[0]) / 2)\n    cY = int((extreme_top[1] + extreme_bottom[1]) / 2)\n\n    # find the maximum euclidean distance between the center of the palm\n    # and the most extreme points of the convex hull\n    distance = pairwise.euclidean_distances([(cX, cY)], Y=[extreme_left, extreme_right, extreme_top, extreme_bottom])[0]\n    maximum_distance = distance[distance.argmax()]\n\n    # calculate the radius of the circle with 80% of the max euclidean distance obtained\n    radius = int(0.8 * maximum_distance)\n\n    # find the circumference of the circle\n    circumference = (2 * np.pi * radius)\n\n    # take out the circular region of interest which has \n    # the palm and the fingers\n    circular_roi = np.zeros(thresholded.shape[:2], dtype="uint8")\n\t\n    # draw the circular ROI\n    cv2.circle(circular_roi, (cX, cY), radius, 255, 1)\n\n    # take bit-wise AND between thresholded hand using the circular ROI as the mask\n    # which gives the cuts obtained using mask on the thresholded hand image\n    circular_roi = cv2.bitwise_and(thresholded, thresholded, mask=circular_roi)\n\n    # compute the contours in the circular ROI\n    (cnts, _) = cv2.findContours(circular_roi.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n\n    # initalize the finger count\n    count = 0\n\n    # loop through the contours found\n    for c in cnts:\n        # compute the bounding box of the contour\n        (x, y, w, h) = cv2.boundingRect(c)\n\n        # increment the count of fingers only if -\n        # 1. The contour region is not the wrist (bottom area)\n        # 2. The number of points along the contour does not exceed\n        #     25% of the circumference of the circular ROI\n        if ((cY + (cY * 0.25)) &gt; (y + h)) and ((circumference * 0.25) &gt; c.shape[0]):\n            count += 1\n\n    return count\n\n#-----------------\n# MAIN FUNCTION\n#-----------------\nif __name__ == "__main__":\n\n    # initialize accumulated weight\n    accumWeight = 0.5\n\n    # get the reference to the webcam\n    camera = cv2.VideoCapture(0)\n\n    # region of interest (ROI) coordinates\n    top, right, bottom, left = 10, 350, 225, 590\n\n    # initialize num of frames\n    num_frames = 0\n\n    # calibration indicator\n    calibrated = False\n\n    # In Data Table\n    \n\n    # keep looping, until interrupted\n    while(True):\n        # get the current frame\n        (grabbed, frame) = camera.read()\n\n        # resize the frame\n        frame = imutils.resize(frame, width=700)\n\n        # flip the frame so that it is not the mirror view\n        frame = cv2.flip(frame, 1)\n\n        # clone the frame\n        clone = frame.copy()\n\n        # get the height and width of the frame\n        (height, width) = frame.shape[:2]\n\n        # get the ROI\n        roi = frame[top:bottom, right:left]\n\n        # convert the roi to grayscale and blur it\n        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n        gray = cv2.GaussianBlur(gray, (7, 7), 0)\n\n        # to get the background, keep looking till a threshold is reached\n        # so that our weighted average model gets calibrated\n        if num_frames &lt; 30:\n            run_avg(gray, accumWeight)\n            if num_frames == 1:\n                print("[STATUS] please wait! calibrating...")\n            elif num_frames == 29:\n                print("[STATUS] calibration successfull...")\n        else:\n            # segment the hand region\n            hand = segment(gray)\n\n            # check whether hand region is segmented\n            if hand is not None:\n                # if yes, unpack the thresholded image and\n                # segmented region\n                (thresholded, segmented) = hand\n\n                # draw the segmented region and display the frame\n                cv2.drawContours(clone, [segmented + (right, top)], -1, (0, 0, 255))\n\n                # count the number of fingers\n        ## #       fingers = count(thresholded, segmented)\n\n                cv2.putText(clone, str(classifier), (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n                \n                # show the thresholded image\n                cv2.imshow("Thesholded", thresholded)\n\n        # draw the segmented hand\n        cv2.rectangle(clone, (left, top), (right, bottom), (0,255,0), 2)\n\n        # increment the number of frames\n        num_frames += 1\n\n        # display the frame with segmented hand\n        cv2.imshow("Video Feed", clone)\n\n        # observe the keypress by the user~\n        keypress = cv2.waitKey(1) &amp; 0xFF\n\n        # if the user pressed "q", then stop looping\n        if keypress == ord("q"):\n            break\n\n    # free up memory\n    camera.release()\n    cv2.destroyAllWindows()', 'filename': None}, {'name': 'New script', 'script': '#------------------------------------------------------------\n# SEGMENT, RECOGNIZE and COUNT fingers from a video sequence\n#------------------------------------------------------------\n\n# organize imports\nimport cv2\nimport imutils\nimport numpy as np\nfrom sklearn.metrics import pairwise\n\n\n# global variables\nbg = None\n\n#--------------------------------------------------\n# To find the running average over the background\n#--------------------------------------------------\ndef run_avg(image, accumWeight):\n    global bg\n    # initialize the background\n    if bg is None:\n        bg = image.copy().astype("float")\n        return\n\n    # compute weighted average, accumulate it and update the background\n    cv2.accumulateWeighted(image, bg, accumWeight)\n\n#---------------------------------------------\n# To segment the region of hand in the image\n#---------------------------------------------\ndef segment(image, threshold=24):\n    global bg\n    # find the absolute difference between background and current frame\n    diff = cv2.absdiff(bg.astype("uint8"), image)\n\n    # threshold the diff image so that we get the foreground\n    thresholded = cv2.threshold(diff, threshold, 128, cv2.THRESH_BINARY)[1]\n\n    # get the contours in the thresholded image\n    (cnts, _) = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    # return None, if no contours detected\n    if len(cnts) == 0:\n        return\n    else:\n        # based on contour area, get the maximum contour which is the hand\n        segmented = max(cnts, key=cv2.contourArea)\n        return (thresholded, segmented)\n\n#--------------------------------------------------------------\n# To count the number of fingers in the segmented hand region\n#--------------------------------------------------------------\ndef count(thresholded, segmented):\n    # find the convex hull of the segmented hand region\n    chull = cv2.convexHull(segmented)\n\n    # find the most extreme points in the convex hull\n    extreme_top    = tuple(chull[chull[:, :, 1].argmin()][0])\n    extreme_bottom = tuple(chull[chull[:, :, 1].argmax()][0])\n    extreme_left   = tuple(chull[chull[:, :, 0].argmin()][0])\n    extreme_right  = tuple(chull[chull[:, :, 0].argmax()][0])\n\n    # find the center of the palm\n    cX = int((extreme_left[0] + extreme_right[0]) / 2)\n    cY = int((extreme_top[1] + extreme_bottom[1]) / 2)\n\n    # find the maximum euclidean distance between the center of the palm\n    # and the most extreme points of the convex hull\n    distance = pairwise.euclidean_distances([(cX, cY)], Y=[extreme_left, extreme_right, extreme_top, extreme_bottom])[0]\n    maximum_distance = distance[distance.argmax()]\n\n    # calculate the radius of the circle with 80% of the max euclidean distance obtained\n    radius = int(0.8 * maximum_distance)\n\n    # find the circumference of the circle\n    circumference = (2 * np.pi * radius)\n\n    # take out the circular region of interest which has \n    # the palm and the fingers\n    circular_roi = np.zeros(thresholded.shape[:2], dtype="uint8")\n\t\n    # draw the circular ROI\n    cv2.circle(circular_roi, (cX, cY), radius, 255, 1)\n\n    # take bit-wise AND between thresholded hand using the circular ROI as the mask\n    # which gives the cuts obtained using mask on the thresholded hand image\n    circular_roi = cv2.bitwise_and(thresholded, thresholded, mask=circular_roi)\n\n    # compute the contours in the circular ROI\n    (cnts, _) = cv2.findContours(circular_roi.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n\n    # initalize the finger count\n    count = 0\n\n    # loop through the contours found\n    for c in cnts:\n        # compute the bounding box of the contour\n        (x, y, w, h) = cv2.boundingRect(c)\n\n        # increment the count of fingers only if -\n        # 1. The contour region is not the wrist (bottom area)\n        # 2. The number of points along the contour does not exceed\n        #     25% of the circumference of the circular ROI\n        if ((cY + (cY * 0.25)) &gt; (y + h)) and ((circumference * 0.25) &gt; c.shape[0]):\n            count += 1\n\n    return count\n\n#-----------------\n# MAIN FUNCTION\n#-----------------\nif __name__ == "__main__":\n\n    # initialize accumulated weight\n    accumWeight = 0.5\n\n    # get the reference to the webcam\n    camera = cv2.VideoCapture(0)\n\n    # region of interest (ROI) coordinates\n    top, right, bottom, left = 10, 350, 225, 590\n\n    # initialize num of frames\n    num_frames = 0\n\n    # calibration indicator\n    calibrated = False\n\n    # In Data Table\n    \n\n    # keep looping, until interrupted\n    while(True):\n        # get the current frame\n        (grabbed, frame) = camera.read()\n\n        # resize the frame\n        frame = imutils.resize(frame, width=700)\n\n        # flip the frame so that it is not the mirror view\n        frame = cv2.flip(frame, 1)\n\n        # clone the frame\n        clone = frame.copy()\n\n        # get the height and width of the frame\n        (height, width) = frame.shape[:2]\n\n        # get the ROI\n        roi = frame[top:bottom, right:left]\n\n        # convert the roi to grayscale and blur it\n        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n        gray = cv2.GaussianBlur(gray, (7, 7), 0)\n\n        # to get the background, keep looking till a threshold is reached\n        # so that our weighted average model gets calibrated\n        if num_frames &lt; 30:\n            run_avg(gray, accumWeight)\n            if num_frames == 1:\n                print("[STATUS] please wait! calibrating...")\n            elif num_frames == 29:\n                print("[STATUS] calibration successfull...")\n        else:\n            # segment the hand region\n            hand = segment(gray)\n\n            # check whether hand region is segmented\n            if hand is not None:\n                # if yes, unpack the thresholded image and\n                # segmented region\n                (thresholded, segmented) = hand\n\n                # draw the segmented region and display the frame\n                cv2.drawContours(clone, [segmented + (right, top)], -1, (0, 0, 255))\n\n                # count the number of fingers\n        ## #       fingers = count(thresholded, segmented)\n\n                cv2.putText(clone, str(classifier), (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n                \n                # show the thresholded image\n                cv2.imshow("Thesholded", thresholded)\n\n        # draw the segmented hand\n        cv2.rectangle(clone, (left, top), (right, bottom), (0,255,0), 2)\n\n        # increment the number of frames\n        num_frames += 1\n\n        # display the frame with segmented hand\n        cv2.imshow("Video Feed", clone)\n\n        # observe the keypress by the user~\n        keypress = cv2.waitKey(1) &amp; 0xFF\n\n        # if the user pressed "q", then stop looping\n        if keypress == ord("q"):\n            break\n\n    # free up memory\n    camera.release()\n    cv2.destroyAllWindows()', 'filename': None}], 'scriptText': '#------------------------------------------------------------\n# SEGMENT, RECOGNIZE and COUNT fingers from a video sequence\n#------------------------------------------------------------\n\n# organize imports\nimport cv2\nimport imutils\nimport numpy as np\nfrom sklearn.metrics import pairwise\n\n\n# global variables\nbg = None\n\n#--------------------------------------------------\n# To find the running average over the background\n#--------------------------------------------------\ndef run_avg(image, accumWeight):\n    global bg\n    # initialize the background\n    if bg is None:\n        bg = image.copy().astype("float")\n        return\n\n    # compute weighted average, accumulate it and update the background\n    cv2.accumulateWeighted(image, bg, accumWeight)\n\n#---------------------------------------------\n# To segment the region of hand in the image\n#---------------------------------------------\ndef segment(image, threshold=24):\n    global bg\n    # find the absolute difference between background and current frame\n    diff = cv2.absdiff(bg.astype("uint8"), image)\n\n    # threshold the diff image so that we get the foreground\n    thresholded = cv2.threshold(diff, threshold, 128, cv2.THRESH_BINARY)[1]\n\n    # get the contours in the thresholded image\n    (cnts, _) = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    # return None, if no contours detected\n    if len(cnts) == 0:\n        return\n    else:\n        # based on contour area, get the maximum contour which is the hand\n        segmented = max(cnts, key=cv2.contourArea)\n        return (thresholded, segmented)\n\n#--------------------------------------------------------------\n# To count the number of fingers in the segmented hand region\n#--------------------------------------------------------------\ndef count(thresholded, segmented):\n    # find the convex hull of the segmented hand region\n    chull = cv2.convexHull(segmented)\n\n    # find the most extreme points in the convex hull\n    extreme_top    = tuple(chull[chull[:, :, 1].argmin()][0])\n    extreme_bottom = tuple(chull[chull[:, :, 1].argmax()][0])\n    extreme_left   = tuple(chull[chull[:, :, 0].argmin()][0])\n    extreme_right  = tuple(chull[chull[:, :, 0].argmax()][0])\n\n    # find the center of the palm\n    cX = int((extreme_left[0] + extreme_right[0]) / 2)\n    cY = int((extreme_top[1] + extreme_bottom[1]) / 2)\n\n    # find the maximum euclidean distance between the center of the palm\n    # and the most extreme points of the convex hull\n    distance = pairwise.euclidean_distances([(cX, cY)], Y=[extreme_left, extreme_right, extreme_top, extreme_bottom])[0]\n    maximum_distance = distance[distance.argmax()]\n\n    # calculate the radius of the circle with 80% of the max euclidean distance obtained\n    radius = int(0.8 * maximum_distance)\n\n    # find the circumference of the circle\n    circumference = (2 * np.pi * radius)\n\n    # take out the circular region of interest which has \n    # the palm and the fingers\n    circular_roi = np.zeros(thresholded.shape[:2], dtype="uint8")\n\t\n    # draw the circular ROI\n    cv2.circle(circular_roi, (cX, cY), radius, 255, 1)\n\n    # take bit-wise AND between thresholded hand using the circular ROI as the mask\n    # which gives the cuts obtained using mask on the thresholded hand image\n    circular_roi = cv2.bitwise_and(thresholded, thresholded, mask=circular_roi)\n\n    # compute the contours in the circular ROI\n    (cnts, _) = cv2.findContours(circular_roi.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n\n    # initalize the finger count\n    count = 0\n\n    # loop through the contours found\n    for c in cnts:\n        # compute the bounding box of the contour\n        (x, y, w, h) = cv2.boundingRect(c)\n\n        # increment the count of fingers only if -\n        # 1. The contour region is not the wrist (bottom area)\n        # 2. The number of points along the contour does not exceed\n        #     25% of the circumference of the circular ROI\n        if ((cY + (cY * 0.25)) &gt; (y + h)) and ((circumference * 0.25) &gt; c.shape[0]):\n            count += 1\n\n    return count\n\n#-----------------\n# MAIN FUNCTION\n#-----------------\nif __name__ == "__main__":\n\n    # initialize accumulated weight\n    accumWeight = 0.5\n\n    # get the reference to the webcam\n    camera = cv2.VideoCapture(0)\n\n    # region of interest (ROI) coordinates\n    top, right, bottom, left = 10, 350, 225, 590\n\n    # initialize num of frames\n    num_frames = 0\n\n    # calibration indicator\n    calibrated = False\n\n    # In Data Table\n    \n\n    # keep looping, until interrupted\n    while(True):\n        # get the current frame\n        (grabbed, frame) = camera.read()\n\n        # resize the frame\n        frame = imutils.resize(frame, width=700)\n\n        # flip the frame so that it is not the mirror view\n        frame = cv2.flip(frame, 1)\n\n        # clone the frame\n        clone = frame.copy()\n\n        # get the height and width of the frame\n        (height, width) = frame.shape[:2]\n\n        # get the ROI\n        roi = frame[top:bottom, right:left]\n\n        # convert the roi to grayscale and blur it\n        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n        gray = cv2.GaussianBlur(gray, (7, 7), 0)\n\n        # to get the background, keep looking till a threshold is reached\n        # so that our weighted average model gets calibrated\n        if num_frames &lt; 30:\n            run_avg(gray, accumWeight)\n            if num_frames == 1:\n                print("[STATUS] please wait! calibrating...")\n            elif num_frames == 29:\n                print("[STATUS] calibration successfull...")\n        else:\n            # segment the hand region\n            hand = segment(gray)\n\n            # check whether hand region is segmented\n            if hand is not None:\n                # if yes, unpack the thresholded image and\n                # segmented region\n                (thresholded, segmented) = hand\n\n                # draw the segmented region and display the frame\n                cv2.drawContours(clone, [segmented + (right, top)], -1, (0, 0, 255))\n\n                # count the number of fingers\n        ## #       fingers = count(thresholded, segmented)\n\n                cv2.putText(clone, str(classifier), (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n                \n                # show the thresholded image\n                cv2.imshow("Thesholded", thresholded)\n\n        # draw the segmented hand\n        cv2.rectangle(clone, (left, top), (right, bottom), (0,255,0), 2)\n\n        # increment the number of frames\n        num_frames += 1\n\n        # display the frame with segmented hand\n        cv2.imshow("Video Feed", clone)\n\n        # observe the keypress by the user~\n        keypress = cv2.waitKey(1) &amp; 0xFF\n\n        # if the user pressed "q", then stop looping\n        if keypress == ord("q"):\n            break\n\n    # free up memory\n    camera.release()\n    cv2.destroyAllWindows()', 'splitterState': b'\x00\x00\x00\xff\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x02\xe3\x00\x00\x00\xe4\x01\xff\xff\xff\xff\x01\x00\x00\x00\x02\x00', '__version__': 2}</properties>
		<properties format="literal" node_id="3">{'_auto_apply': True, 'cb_embedder_current_id': 0, 'cb_image_attr_current_id': 0, 'controlAreaVisible': True, 'savedWidgetGeometry': None, '__version__': 1}</properties>
		<properties format="literal" node_id="4">{'C_index': 61, 'auto_apply': True, 'controlAreaVisible': True, 'learner_name': '', 'penalty_type': 1, 'savedWidgetGeometry': b'\x01\xd9\xd0\xcb\x00\x03\x00\x00\x00\x00\x03V\x00\x00\x01t\x00\x00\x04*\x00\x00\x02\x87\x00\x00\x03W\x00\x00\x01\x93\x00\x00\x04)\x00\x00\x02\x86\x00\x00\x00\x00\x00\x00\x00\x00\x07\x80\x00\x00\x03W\x00\x00\x01\x93\x00\x00\x04)\x00\x00\x02\x86', '__version__': 1}</properties>
		<properties format="literal" node_id="5">{'append_predictions': True, 'append_probabilities': False, 'autocommit': True, 'controlAreaVisible': True, 'savedWidgetGeometry': None, 'selected_learner': [0], 'selected_quantity': 0, '__version__': 1, 'context_settings': []}</properties>
		<properties format="pickle" node_id="6">gASVCgEAAAAAAAB9lCiMEmNvbnRyb2xBcmVhVmlzaWJsZZSIjBNzYXZlZFdpZGdldEdlb21ldHJ5
lENCAdnQywADAAAAAAJoAAABHQAABRcAAALeAAACaQAAATwAAAUWAAAC3QAAAAAAAAAAB4AAAAJp
AAABPAAABRYAAALdlIwJc2VsZWN0aW9ulF2UjAtzY29yZV90YWJsZZR9lIwMc2hvd25fc2NvcmVz
lI+UKIwDQVVDlIwDTVNFlIwCQ0GUjAlQcmVjaXNpb26UjAZSZWNhbGyUjAJSMpSMBFJNU0WUjAJG
MZSMA01BRZSQc4wLX192ZXJzaW9uX1+USwGMEGNvbnRleHRfc2V0dGluZ3OUXZR1Lg==
</properties>
	</node_properties>
	<session_state>
		<window_groups />
	</session_state>
</scheme>
